{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "630db218",
   "metadata": {},
   "source": [
    "# Exploratory & A/B Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e00674b",
   "metadata": {},
   "source": [
    "## 1. Subject Line A/B Performance\n",
    "\n",
    "* Q1: Do variant A vs B subject lines produce statistically significant differences in open rate?\n",
    "\n",
    "* Q2:  Is the effect of subject variant on open rate consistent across segments and channels, or explained by confounders\n",
    "\n",
    "## 2. Timing & Frequency Effects\n",
    "\n",
    "* Q3: Does send time (hour/day) affect open/CTR?\n",
    "\n",
    "* Q4: How does customer engagement fatigue—measured by cumulative opens and inactivity periods—influence unsubscribe rates and purchase likelihood?\n",
    "\n",
    "## 3. Segment-Level Insights\n",
    "\n",
    "* Q5: Which customer segments (RFM quintiles, new vs. repeat) respond best to promotions vs. newsletters?\n",
    "\n",
    "* Q6: What is the interplay between purchase frequency and email engagement?\n",
    "\n",
    "* Q7: For engaged customers (opened/clicked), what is the uplift in next-30-day order rate and average order value vs. non-openers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae3fd313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "email_campaign_df = pd.read_csv('../data/email_campaigns.csv')\n",
    "for col in ['campaign_id', 'channel', 'segment', 'customer_id', 'recipient_name', 'send_timestamp', 'subject_variant', 'subject_line', 'device']:\n",
    "    email_campaign_df[col] = email_campaign_df[col].astype('string')\n",
    "\n",
    "email_engagement_df = pd.read_csv('../data/email_engagement.csv')\n",
    "for col in ['campaign_id', 'customer_id', 'send_timestamp', 'subject_variant']:\n",
    "    email_engagement_df[col] = email_engagement_df[col].astype('string')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert timestamps\n",
    "email_campaign_df['send_timestamp'] = pd.to_datetime(email_campaign_df['send_timestamp'], errors='coerce')\n",
    "email_engagement_df['send_timestamp'] = pd.to_datetime(email_engagement_df['send_timestamp'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37574c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% sample of 10 million rows \n",
    "email_campaign_df = email_campaign_df.sample(frac=0.10, random_state=42)\n",
    "filtered_keys = email_campaign_df[['campaign_id', 'customer_id', 'send_timestamp']]\n",
    "\n",
    "email_engagement_df = email_engagement_df.merge(\n",
    "    filtered_keys,\n",
    "    on=['campaign_id', 'customer_id', 'send_timestamp'],\n",
    "    how='inner'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17884c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>send_timestamp</th>\n",
       "      <th>opened</th>\n",
       "      <th>clicked</th>\n",
       "      <th>unsubscribed</th>\n",
       "      <th>purchase</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1568632</td>\n",
       "      <td>1.568632e+06</td>\n",
       "      <td>1.568632e+06</td>\n",
       "      <td>1.568632e+06</td>\n",
       "      <td>1.568632e+06</td>\n",
       "      <td>1.568632e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2025-01-06 06:16:27.767429888</td>\n",
       "      <td>1.100730e-01</td>\n",
       "      <td>4.000301e-03</td>\n",
       "      <td>3.602502e-03</td>\n",
       "      <td>4.666486e-04</td>\n",
       "      <td>1.622352e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2024-07-01 10:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2024-10-08 10:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2024-12-24 10:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2025-04-16 10:00:00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2025-06-30 19:00:00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.352831e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.129808e-01</td>\n",
       "      <td>6.312132e-02</td>\n",
       "      <td>5.991266e-02</td>\n",
       "      <td>2.159702e-02</td>\n",
       "      <td>1.137975e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      send_timestamp        opened       clicked  \\\n",
       "count                        1568632  1.568632e+06  1.568632e+06   \n",
       "mean   2025-01-06 06:16:27.767429888  1.100730e-01  4.000301e-03   \n",
       "min              2024-07-01 10:00:00  0.000000e+00  0.000000e+00   \n",
       "25%              2024-10-08 10:00:00  0.000000e+00  0.000000e+00   \n",
       "50%              2024-12-24 10:00:00  0.000000e+00  0.000000e+00   \n",
       "75%              2025-04-16 10:00:00  0.000000e+00  0.000000e+00   \n",
       "max              2025-06-30 19:00:00  1.000000e+00  1.000000e+00   \n",
       "std                              NaN  3.129808e-01  6.312132e-02   \n",
       "\n",
       "       unsubscribed      purchase       revenue  \n",
       "count  1.568632e+06  1.568632e+06  1.568632e+06  \n",
       "mean   3.602502e-03  4.666486e-04  1.622352e-02  \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "max    1.000000e+00  1.000000e+00  3.352831e+02  \n",
       "std    5.991266e-02  2.159702e-02  1.137975e+00  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_engagement_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479a9bd",
   "metadata": {},
   "source": [
    "## 1. Subject Line A/B Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39d4d7",
   "metadata": {},
   "source": [
    "### Q1: Do variant A vs B subject lines produce statistically significant differences in open rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84188fcf",
   "metadata": {},
   "source": [
    "#### Statistical test (e.g., two-proportion z-test) for significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6331743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-Stat: -13.157596463000761, p-value: 1.539042593237991e-39\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "\n",
    "# Aggregate open counts by variant \n",
    "\n",
    "agg= (email_engagement_df\n",
    "      .groupby('subject_variant')['opened']\n",
    "      .agg(['sum','count'])\n",
    "      )\n",
    "\n",
    "\n",
    "## Variant A vs Variant B\n",
    "\n",
    "open = agg.loc[['A','B'],'sum'] \n",
    "\n",
    "totals = agg.loc[['A','B'],'count']\n",
    "\n",
    "\n",
    "# two-propertion z test\n",
    "stats, pval = proportions_ztest(open,totals)\n",
    "\n",
    "print(f\"Z-Stat: {stats}, p-value: {pval}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278789c",
   "metadata": {},
   "source": [
    "#### Effect size calculation (difference in open rate) for praticial impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89dcfb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect size (A-B): -0.008399\n",
      "95% CI: (-0.00965, -0.00715)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.proportion import confint_proportions_2indep\n",
    "\n",
    "#open rates \n",
    "rate_A = open.iloc[0]/totals.iloc[0]\n",
    "rate_B = open.iloc[1]/totals.iloc[1]\n",
    "\n",
    "effect_size= rate_A-rate_B\n",
    "\n",
    "\n",
    "## confidence interval for difference \n",
    "# method = wald or score can be used \n",
    "\n",
    "lower,upper = confint_proportions_2indep(open.iloc[0],totals.iloc[0],open.iloc[1],totals.iloc[1],method='score')\n",
    "\n",
    "print(f\"Effect size (A-B): {effect_size:5f}\")\n",
    "\n",
    "print(f\"95% CI: ({lower:.5f}, {upper:.5f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286c866",
   "metadata": {},
   "source": [
    "### *Summary for Subject Line A/B Performance*\n",
    "\n",
    "The statistical analysis shows a highly significant difference in open rates between subject line variants A and B.\n",
    "\n",
    "- Two-proportion z-test result: Z = -32.86, p-value ≈ 0, indicating the difference is not due to chance.\n",
    "- Effect size (A - B): -0.00937, meaning variant A's open rate is approximately 0.9% lower than variant B's.\n",
    "-  95% confidence interval for the difference: (-0.00993, -0.00881), confirming precision and that the difference is consistently below zero.\n",
    "\n",
    "Conclusion: Subject line variant has a statistically significant and measurable impact on open rate. The difference is precise and robust given the large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8c9b34",
   "metadata": {},
   "source": [
    "#### Q2: Is the effect of subject variant on open rate consistent across segments and channels, or explained by confounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "671a826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge email_campaign_df with email_engagement_df to add channel and segment info to engagement data\n",
    "# Drop duplicates to keep unique campaign_id with channel and segment\n",
    "campaign_agg = email_campaign_df[['campaign_id', 'channel', 'segment']].drop_duplicates(subset=['campaign_id']).copy()\n",
    "\n",
    "# Merge aggregated campaign info with engagement data\n",
    "merged_df = email_engagement_df.merge(campaign_agg, on='campaign_id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89485806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['campaign_id', 'customer_id', 'send_timestamp', 'subject_variant',\n",
       "       'opened', 'clicked', 'unsubscribed', 'purchase', 'revenue', 'channel',\n",
       "       'segment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f94ddc",
   "metadata": {},
   "source": [
    " #### logistic regression to model open probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75cff956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.353403\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 opened   No. Observations:               987776\n",
      "Model:                          Logit   Df Residuals:                   987756\n",
      "Method:                           MLE   Df Model:                           19\n",
      "Date:                Fri, 08 Aug 2025   Pseudo R-squ.:                0.001026\n",
      "Time:                        11:53:44   Log-Likelihood:            -3.4908e+05\n",
      "converged:                       True   LL-Null:                   -3.4944e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                9.891e-140\n",
      "==================================================================================================================\n",
      "                                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                         -2.2431      0.036    -61.856      0.000      -2.314      -2.172\n",
      "subject_variant[T.B]                               0.1327      0.050      2.656      0.008       0.035       0.231\n",
      "channel[T.loyalty]                                 0.2051      0.043      4.759      0.000       0.121       0.290\n",
      "channel[T.newsletter]                              0.0404      0.036      1.114      0.265      -0.031       0.111\n",
      "channel[T.promo]                                   0.2118      0.036      5.933      0.000       0.142       0.282\n",
      "channel[T.survey]                                  0.1072      0.056      1.916      0.055      -0.002       0.217\n",
      "channel[T.transactional]                           0.1107      0.038      2.897      0.004       0.036       0.186\n",
      "segment[T.inactive]                                0.0021      0.016      0.131      0.896      -0.029       0.033\n",
      "segment[T.new_subscriber]                          0.0118      0.018      0.659      0.510      -0.023       0.047\n",
      "segment[T.occasional]                              0.0149      0.014      1.077      0.282      -0.012       0.042\n",
      "segment[T.repeat_buyer]                           -0.0271      0.013     -2.054      0.040      -0.053      -0.001\n",
      "subject_variant[T.B]:channel[T.loyalty]            0.0080      0.059      0.136      0.892      -0.108       0.124\n",
      "subject_variant[T.B]:channel[T.newsletter]        -0.0379      0.050     -0.759      0.448      -0.136       0.060\n",
      "subject_variant[T.B]:channel[T.promo]             -0.0743      0.049     -1.512      0.131      -0.171       0.022\n",
      "subject_variant[T.B]:channel[T.survey]             0.0095      0.077      0.123      0.902      -0.141       0.160\n",
      "subject_variant[T.B]:channel[T.transactional]     -0.0537      0.053     -1.019      0.308      -0.157       0.050\n",
      "subject_variant[T.B]:segment[T.inactive]           0.0051      0.022      0.230      0.818      -0.038       0.048\n",
      "subject_variant[T.B]:segment[T.new_subscriber]     0.0026      0.025      0.105      0.917      -0.046       0.052\n",
      "subject_variant[T.B]:segment[T.occasional]         0.0207      0.019      1.077      0.281      -0.017       0.058\n",
      "subject_variant[T.B]:segment[T.repeat_buyer]       0.0065      0.018      0.355      0.722      -0.030       0.043\n",
      "==================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "## prepare merged df with nesessary columns \n",
    "## ensuring categorical variables are treated as such \n",
    "\n",
    "merged_df['subject_variant']= merged_df['subject_variant'].astype('category')\n",
    "merged_df['channel']= merged_df['channel'].astype('category')\n",
    "merged_df['segment']= merged_df['segment'].astype('category')\n",
    "\n",
    "\n",
    "# Logistic regression formula with interactions\n",
    "formula = 'opened ~ subject_variant + channel + segment + subject_variant:channel + subject_variant:segment'\n",
    "\n",
    "\n",
    "# fit model using statsmodel \n",
    "model = smf.logit(formula=formula, data=merged_df).fit()\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40553b",
   "metadata": {},
   "source": [
    "#### *Model Significance Order and Conclusion*\n",
    "\n",
    "Order of predictors by statistical significance (lowest to highest p-value):\n",
    "\n",
    "1. channel[T.promo] (p ≈ 0.000, z=5.933) - strongest positive effect  \n",
    "2. channel[T.loyalty] (p ≈ 0.000, z=4.759)  \n",
    "3. channel[T.transactional] (p=0.004, z=2.897)  \n",
    "4. subject_variant[T.B] (p=0.008, z=2.656)  \n",
    "5. channel[T.survey] (p=0.055, z=1.921) - borderline  \n",
    "6. channel[T.newsletter] (p=0.265, z=1.119) - not significant  \n",
    "7. segment[...] terms – all p > 0.1, not significant  \n",
    "8. interaction terms – all p > 0.1, not significant  \n",
    "\n",
    "**Conclusion:**  \n",
    "\n",
    "The promotional, loyalty, and transactional channels significantly increase the odds of email opens, with promo having the largest effect.\n",
    "\n",
    "Subject variant B also significantly increases open likelihood but with a smaller effect than these top channels.\n",
    "\n",
    "All segment and interaction effects are statistically insignificant, while the newsletter and survey channels show weak or borderline impact on open rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c66762",
   "metadata": {},
   "source": [
    "## 2. Timing & Frequency Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb0833",
   "metadata": {},
   "source": [
    "### Q3: Does send time (hour/day) affect open/CTR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "388358c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (merged_df\n",
    "           .groupby([merged_df['send_timestamp'].dt.dayofweek.rename('day_sent'),\n",
    "                     merged_df['send_timestamp'].dt.hour.rename('hour_sent')])\n",
    "           .agg(opened_sum=('opened', 'sum'),\n",
    "                clicked_sum=('clicked', 'sum'),\n",
    "                email_sent=('campaign_id', 'count'))\n",
    "          ).reset_index()\n",
    "\n",
    "grouped['open_rate'] = grouped['opened_sum'] / grouped['email_sent']\n",
    "grouped['ctr'] = grouped['clicked_sum'] / grouped['email_sent']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ca19d",
   "metadata": {},
   "source": [
    "#### Logistic regression to test impact of send day and hour on email open probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "684f66b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['day_sent']= merged_df['send_timestamp'].dt.dayofweek\n",
    "merged_df['hour_sent']= merged_df['send_timestamp'].dt.hour\n",
    "\n",
    "\n",
    "logit_df = merged_df[['opened', 'day_sent', 'hour_sent']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c27dc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.339970\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 opened   No. Observations:                15686\n",
      "Model:                          Logit   Df Residuals:                    15668\n",
      "Method:                           MLE   Df Model:                           17\n",
      "Date:                Fri, 08 Aug 2025   Pseudo R-squ.:                0.002885\n",
      "Time:                        11:56:08   Log-Likelihood:                -5332.8\n",
      "converged:                       True   LL-Null:                       -5348.2\n",
      "Covariance Type:            nonrobust   LLR p-value:                   0.02079\n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept          -1.9455      0.199     -9.778      0.000      -2.335      -1.555\n",
      "day_sent[T.1]      -0.1273      0.097     -1.317      0.188      -0.317       0.062\n",
      "day_sent[T.2]       0.0486      0.092      0.529      0.597      -0.132       0.229\n",
      "day_sent[T.3]      -0.0156      0.095     -0.164      0.870      -0.202       0.171\n",
      "day_sent[T.4]      -0.0908      0.094     -0.963      0.336      -0.276       0.094\n",
      "day_sent[T.5]      -0.1414      0.118     -1.194      0.233      -0.373       0.091\n",
      "day_sent[T.6]      -0.0678      0.116     -0.585      0.559      -0.295       0.160\n",
      "hour_sent[T.10]    -0.0719      0.195     -0.369      0.712      -0.453       0.310\n",
      "hour_sent[T.11]     0.0200      0.194      0.103      0.918      -0.360       0.400\n",
      "hour_sent[T.12]    -0.2956      0.244     -1.211      0.226      -0.774       0.183\n",
      "hour_sent[T.13]    -0.0435      0.235     -0.185      0.853      -0.503       0.416\n",
      "hour_sent[T.14]    -0.1624      0.241     -0.673      0.501      -0.635       0.310\n",
      "hour_sent[T.15]     0.1922      0.229      0.840      0.401      -0.256       0.641\n",
      "hour_sent[T.16]     0.0699      0.229      0.305      0.760      -0.379       0.519\n",
      "hour_sent[T.17]    -0.1781      0.241     -0.738      0.460      -0.651       0.295\n",
      "hour_sent[T.18]    -0.3093      0.196     -1.578      0.115      -0.694       0.075\n",
      "hour_sent[T.19]    -0.1739      0.195     -0.892      0.373      -0.556       0.208\n",
      "hour_sent[T.20]    -0.2502      0.246     -1.017      0.309      -0.733       0.232\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Random 1% sample (≈ 100k rows) to fit model\n",
    "sample_df = merged_df.sample(frac=0.01, random_state=42)\n",
    "\n",
    "# Convert to categorical\n",
    "sample_df['day_sent'] = sample_df['day_sent'].astype('category')\n",
    "sample_df['hour_sent'] = sample_df['hour_sent'].astype('category')\n",
    "\n",
    "# Fit logistic regression\n",
    "formula = 'opened ~ day_sent + hour_sent'\n",
    "model = smf.logit(formula=formula, data=sample_df)\n",
    "result = model.fit()\n",
    "\n",
    "print(result.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9570c",
   "metadata": {},
   "source": [
    "#### Logistic regression to test impact of send day and hour on email CTR probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa6ca569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Generalized Linear Model Regression Results                      \n",
      "=======================================================================================\n",
      "Dep. Variable:     ['clicked_sum', 'failures']   No. Observations:                   44\n",
      "Model:                                     GLM   Df Residuals:                       26\n",
      "Model Family:                         Binomial   Df Model:                           17\n",
      "Link Function:                           Logit   Scale:                          1.0000\n",
      "Method:                                   IRLS   Log-Likelihood:                -162.36\n",
      "Date:                         Fri, 08 Aug 2025   Deviance:                       35.672\n",
      "Time:                                 11:57:08   Pearson chi2:                     35.6\n",
      "No. Iterations:                              9   Pseudo R-squ. (CS):             0.9974\n",
      "Covariance Type:                     nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept          -5.2805      0.095    -55.436      0.000      -5.467      -5.094\n",
      "day_sent[T.1]      -0.0065      0.046     -0.142      0.887      -0.096       0.083\n",
      "day_sent[T.2]       0.0589      0.045      1.309      0.191      -0.029       0.147\n",
      "day_sent[T.3]       0.0049      0.046      0.106      0.916      -0.085       0.095\n",
      "day_sent[T.4]       0.0267      0.045      0.592      0.554      -0.062       0.115\n",
      "day_sent[T.5]      -0.1479      0.060     -2.479      0.013      -0.265      -0.031\n",
      "day_sent[T.6]      -0.1639      0.059     -2.771      0.006      -0.280      -0.048\n",
      "hour_sent[T.10]     0.0012      0.092      0.013      0.990      -0.180       0.182\n",
      "hour_sent[T.11]    -0.0540      0.092     -0.584      0.559      -0.235       0.127\n",
      "hour_sent[T.12]    -0.2397      0.119     -2.019      0.043      -0.472      -0.007\n",
      "hour_sent[T.13]    -0.3048      0.121     -2.526      0.012      -0.541      -0.068\n",
      "hour_sent[T.14]    -0.3085      0.121     -2.550      0.011      -0.546      -0.071\n",
      "hour_sent[T.15]    -0.3157      0.122     -2.598      0.009      -0.554      -0.078\n",
      "hour_sent[T.16]    -0.2255      0.118     -1.912      0.056      -0.457       0.006\n",
      "hour_sent[T.17]    -0.2667      0.120     -2.231      0.026      -0.501      -0.032\n",
      "hour_sent[T.18]    -0.3768      0.094     -4.015      0.000      -0.561      -0.193\n",
      "hour_sent[T.19]    -0.4345      0.094     -4.619      0.000      -0.619      -0.250\n",
      "hour_sent[T.20]    -0.4362      0.126     -3.475      0.001      -0.682      -0.190\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "grouped['failures'] = grouped['email_sent'] - grouped['clicked_sum']\n",
    "\n",
    "grouped['day_sent'] = grouped['day_sent'].astype('category')\n",
    "grouped['hour_sent'] = grouped['hour_sent'].astype('category')\n",
    "\n",
    "formula = 'clicked_sum + failures ~ day_sent + hour_sent'\n",
    "\n",
    "model = smf.glm(formula=formula, data=grouped, family=sm.families.Binomial())\n",
    "result = model.fit()\n",
    "\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f16da",
   "metadata": {},
   "source": [
    "#### *Summary for open and CTR probability*\n",
    "\n",
    "Conclusion: The model's low pseudo R-squared (0.0025) and coefficients show weak predictive power and inconsistent day/hour effects, suggesting the data is noisy, synthetic, or poorly recorded. The lack of strong positive coefficients means no clear best send time emerges. The dataset or feature engineering likely needs improvement before actionable insights. The data shows Sunday at 00:00 being the most optimal day to send the emails which makes no sense in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1735db28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['campaign_id', 'customer_id', 'send_timestamp', 'subject_variant',\n",
       "       'opened', 'clicked', 'unsubscribed', 'purchase', 'revenue', 'channel',\n",
       "       'segment', 'day_sent', 'hour_sent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f932d",
   "metadata": {},
   "source": [
    "## Q4: How does customer engagement fatigue-measured by cumulative opens and inactivity periods-influence unsubscribe rates and purchase likelihood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "830d9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85182650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['customer_id'] = df['customer_id'].astype('category')\n",
    "\n",
    "df['purchase'] = df['purchase'].astype('int8')\n",
    "df['send_timestamp'] = pd.to_datetime(df['send_timestamp'])\n",
    "\n",
    "## sort the values by customer_id and send timestamp\n",
    "df.sort_values(['customer_id', 'send_timestamp'], inplace=True)\n",
    "\n",
    "## identify first row per customer \n",
    "df['is_first']= df.groupby('customer_id',observed=True).cumcount()==0\n",
    "\n",
    "## has any prior purchase? \n",
    "df['prior_purchase']= ( \n",
    "    df.groupby('customer_id',observed=True)['purchase']\n",
    "    .transform(lambda x: x.shift().cummax().fillna(0)))\n",
    "\n",
    "## assing segments \n",
    "df['segment']='inactive'\n",
    "df.loc[df['is_first'],'segment']= 'new'\n",
    "\n",
    "# if customer is not new and has prior purchase then they are a repat  \n",
    "df.loc[(~df['is_first']) & (df['prior_purchase']>0),'segment']='repeat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e8ab4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['days_since_last_purchase'] = df.groupby('customer_id',observed=True)['send_timestamp'].diff().dt.days.fillna(0)\n",
    "\n",
    "df['days_since_last_purchase_cum'] = df.groupby('customer_id',observed=True)['days_since_last_purchase'].cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be946c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.681011\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:           unsubscribed   No. Observations:                11302\n",
      "Model:                          Logit   Df Residuals:                    11297\n",
      "Method:                           MLE   Df Model:                            4\n",
      "Date:                Fri, 08 Aug 2025   Pseudo R-squ.:                 0.01751\n",
      "Time:                        11:58:35   Log-Likelihood:                -7696.8\n",
      "converged:                       True   LL-Null:                       -7833.9\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.723e-58\n",
      "=======================================================================================================\n",
      "                                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Intercept                              -0.0469      0.020     -2.391      0.017      -0.085      -0.008\n",
      "segment[T.new]                          0.0775      0.212      0.365      0.715      -0.338       0.493\n",
      "segment[T.repeat]                      -0.2115      0.138     -1.530      0.126      -0.482       0.059\n",
      "days_since_last_purchase_cum_scaled    -0.3854      0.029    -13.263      0.000      -0.442      -0.328\n",
      "cumulative_opens_scaled                 0.1192      0.029      4.063      0.000       0.062       0.177\n",
      "=======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Create per-customer cumulative opens\n",
    "df['cumulative_opens'] = (\n",
    "    df.groupby('customer_id', observed=True)['opened']\n",
    "      .cumsum()\n",
    ")\n",
    "\n",
    "# Scale continuous predictors to avoid overflow\n",
    "df['cumulative_opens_scaled'] = (\n",
    "    (df['cumulative_opens'] - df['cumulative_opens'].mean()) /\n",
    "    df['cumulative_opens'].std()\n",
    ")\n",
    "df['days_since_last_purchase_cum_scaled'] = (\n",
    "    (df['days_since_last_purchase_cum'] - df['days_since_last_purchase_cum'].mean()) /\n",
    "    df['days_since_last_purchase_cum'].std()\n",
    ")\n",
    "\n",
    "# Stratified sample for unsubscribe model\n",
    "sample_df_unsub = (\n",
    "    df.groupby('unsubscribed', group_keys=False, observed=True)\n",
    "      .sample(n=min(50000, df['unsubscribed'].value_counts().min()),\n",
    "              random_state=42)\n",
    ")\n",
    "\n",
    "# Stratified sample for purchase model\n",
    "sample_df_purchase = (\n",
    "    df.groupby('purchase', group_keys=False, observed=True)\n",
    "      .sample(n=min(50000, df['purchase'].value_counts().min()),\n",
    "              random_state=42)\n",
    ")\n",
    "\n",
    "# Ensure binary outcomes are int\n",
    "sample_df_unsub['unsubscribed'] = sample_df_unsub['unsubscribed'].astype(int)\n",
    "sample_df_purchase['purchase'] = sample_df_purchase['purchase'].astype(int)\n",
    "\n",
    "# Logistic regression: unsubscribe ~ fatigue/inactivity\n",
    "model_unsub = smf.logit(\n",
    "    formula='unsubscribed ~ days_since_last_purchase_cum_scaled + cumulative_opens_scaled + segment',\n",
    "    data=sample_df_unsub\n",
    ").fit()\n",
    "print(model_unsub.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315af5c",
   "metadata": {},
   "source": [
    "#### *Logistic regression: unsubscribe summary*\n",
    "\n",
    "* days_since_last_purchase_cum_scaled has a strong negative coefficient (p<0.001).  \n",
    "* Longer inactivity periods correlate with a significantly lower likelihood of unsubscribing.  \n",
    "* cumulative_opens_scaled has a strong positive coefficient (p<0.001). Customers with more cumulative opens are significantly more likely to unsubscribe.  \n",
    "* Segment type (new vs repeat) is not significant.  \n",
    "\n",
    "Conclusion: Engagement fatigue is evident. Frequent openers face higher unsubscribe risk, while longer inactivity correlates with reduced risk. Purchase recency appears less influential than cumulative engagement.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3ccdacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.589106\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               purchase   No. Observations:                 1464\n",
      "Model:                          Logit   Df Residuals:                     1460\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Fri, 08 Aug 2025   Pseudo R-squ.:                  0.1501\n",
      "Time:                        12:03:18   Log-Likelihood:                -862.45\n",
      "converged:                       True   LL-Null:                       -1014.8\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.885e-66\n",
      "=======================================================================================================\n",
      "                                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Intercept                              -0.0011      0.061     -0.018      0.986      -0.120       0.118\n",
      "C(segment)[T.repeat]                    0.8306      0.287      2.895      0.004       0.268       1.393\n",
      "days_since_last_purchase_cum_scaled     0.1228      0.087      1.409      0.159      -0.048       0.294\n",
      "cumulative_opens_scaled                 0.9183      0.101      9.108      0.000       0.721       1.116\n",
      "=======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression: purchase ~ fatigue/inactivity\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Collapse 'new' into 'inactive'\n",
    "sample_df_purchase['segment'] = sample_df_purchase['segment'].replace({'new': 'inactive'})\n",
    "\n",
    "# Scale predictors\n",
    "for col in ['days_since_last_purchase_cum', 'cumulative_opens']:\n",
    "    sample_df_purchase[col + '_scaled'] = (\n",
    "        sample_df_purchase[col] - sample_df_purchase[col].mean()\n",
    "    ) / sample_df_purchase[col].std()\n",
    "\n",
    "# Fit logistic regression\n",
    "model_purchase = smf.logit(\n",
    "    formula='purchase ~ days_since_last_purchase_cum_scaled + cumulative_opens_scaled + C(segment)',\n",
    "    data=sample_df_purchase\n",
    ").fit()\n",
    "print(model_purchase.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57242eee",
   "metadata": {},
   "source": [
    "#### *Logistic regression: purchase summary*\n",
    "\n",
    "* cumulative_opens_scaled has a strong positive effect (coef=0.9183, p<0.001).  \n",
    "* Customers with more cumulative opens are significantly more likely to make a purchase.  \n",
    "* Segment repeat has a positive and significant effect (coef=0.8306, p=0.004). Repeat customers are more likely to purchase than new customers.  \n",
    "* days_since_last_purchase_cum_scaled is positive but not statistically significant (p=0.159).  \n",
    "\n",
    "Conclusion: Cumulative engagement strongly drives purchase likelihood. Repeat customers have a higher purchase probability. Purchase recency shows no clear effect here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8581381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0161b7f4",
   "metadata": {},
   "source": [
    "## 3. Segment-Level Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c215c8",
   "metadata": {},
   "source": [
    "#### Q5: Which customer segments (RFM quintiles, new vs. repeat) respond best to promotions vs. newsletters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bb3763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or confirm RFM columns\n",
    "df['recency'] = df['days_since_last_purchase']  # lower better\n",
    "df['frequency'] = df['prior_purchase']  # higher better\n",
    "df['monetary'] = df['revenue']  # higher better\n",
    "\n",
    "# For recency, invert so higher is better for qcut\n",
    "df['recency_inv'] = -df['recency']  # invert to sort recent high\n",
    "\n",
    "# Define quantile labels\n",
    "quantile_labels = [1, 2, 3, 4, 5]\n",
    "\n",
    "df['R_rank'] = df['recency_inv'].rank(method='first')\n",
    "df['R_quintile'] = pd.qcut(df['R_rank'], q=5, labels=quantile_labels, duplicates='drop')\n",
    "\n",
    "df['F_rank'] = df['frequency'].rank(method='first')\n",
    "df['F_quintile'] = pd.qcut(df['F_rank'], q=5, labels=quantile_labels, duplicates='drop')\n",
    "\n",
    "df['M_rank'] = df['monetary'].rank(method='first')\n",
    "df['M_quintile'] = pd.qcut(df['M_rank'], q=5, labels=quantile_labels, duplicates='drop')\n",
    "\n",
    "df['RFM_score'] = df['R_quintile'].astype(str) + df['F_quintile'].astype(str) + df['M_quintile'].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98f7e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for promo and newsletter channels only\n",
    "\n",
    "df_filtered = df[df['channel'].isin(['promo', 'newsletter'])].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9bac423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Remove rows where segment == 'new' and clicked == 0\n",
    "df_filtered_nonzero = df_filtered[~((df_filtered['segment'] == 'new') & (df_filtered['clicked'] == 0))].copy()\n",
    "\n",
    "# Ensure categorical types with limited categories\n",
    "df_filtered_nonzero['channel'] = pd.Categorical(df_filtered_nonzero['channel'], categories=['promo', 'newsletter'])\n",
    "df_filtered_nonzero['segment'] = df_filtered_nonzero['segment'].astype('category')\n",
    "df_filtered_nonzero['rfm_quintile'] = df_filtered_nonzero['RFM_score'].astype('category')\n",
    "\n",
    "# Group by segment, channel, and rfm_quintile aggregating clicks and counts\n",
    "group_cols = ['segment', 'channel', 'rfm_quintile']\n",
    "grouped = (\n",
    "    df_filtered_nonzero\n",
    "    .groupby(group_cols, observed=True)['clicked']\n",
    "    .agg(['sum', 'count'])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate click rate per group\n",
    "grouped['click_rate'] = grouped['sum'] / grouped['count']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04b2d7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  106\n",
      "Model:                            GLM   Df Residuals:                  1314151\n",
      "Model Family:                Binomial   Df Model:                           52\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -27473.\n",
      "Date:                Fri, 08 Aug 2025   Deviance:                       32.433\n",
      "Time:                        12:06:24   Pearson chi2:                     32.5\n",
      "No. Iterations:                   100   Pseudo R-squ. (CS):             0.9901\n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "const                 -5.7904      0.082    -70.887      0.000      -5.950      -5.630\n",
      "segment_new         4.646e-16   5.64e-16      0.824      0.410    -6.4e-16    1.57e-15\n",
      "segment_repeat         1.3938      0.106     13.120      0.000       1.186       1.602\n",
      "channel_newsletter    -0.1884      0.031     -6.149      0.000      -0.248      -0.128\n",
      "rfm_quintile_112      -0.0804      0.591     -0.136      0.892      -1.239       1.078\n",
      "rfm_quintile_115     9.85e-17   7.78e-16      0.127      0.899   -1.43e-15    1.62e-15\n",
      "rfm_quintile_122       0.0867      0.113      0.768      0.442      -0.135       0.308\n",
      "rfm_quintile_123       0.0072      0.380      0.019      0.985      -0.737       0.751\n",
      "rfm_quintile_125    5.154e-16   4.84e-16      1.065      0.287   -4.33e-16    1.46e-15\n",
      "rfm_quintile_133      -0.1358      0.120     -1.132      0.258      -0.371       0.099\n",
      "rfm_quintile_134      -0.3150      0.380     -0.829      0.407      -1.060       0.430\n",
      "rfm_quintile_135     2.26e-17   5.67e-16      0.040      0.968   -1.09e-15    1.13e-15\n",
      "rfm_quintile_144       0.2036      0.111      1.835      0.066      -0.014       0.421\n",
      "rfm_quintile_145       0.9391      0.195      4.815      0.000       0.557       1.321\n",
      "rfm_quintile_151      -0.2853      0.358     -0.797      0.425      -0.987       0.416\n",
      "rfm_quintile_152      -0.4013      0.358     -1.122      0.262      -1.103       0.300\n",
      "rfm_quintile_153      -0.1617      0.358     -0.451      0.652      -0.864       0.541\n",
      "rfm_quintile_154      -0.7402      0.478     -1.550      0.121      -1.676       0.196\n",
      "rfm_quintile_155       0.2069      0.113      1.834      0.067      -0.014       0.428\n",
      "rfm_quintile_211       0.1211      0.104      1.163      0.245      -0.083       0.325\n",
      "rfm_quintile_212       0.1391      0.458      0.304      0.761      -0.759       1.037\n",
      "rfm_quintile_215     3.23e-16   2.59e-16      1.248      0.212   -1.84e-16     8.3e-16\n",
      "rfm_quintile_222       0.0781      0.108      0.721      0.471      -0.134       0.290\n",
      "rfm_quintile_223       0.3226      0.380      0.849      0.396      -0.422       1.067\n",
      "rfm_quintile_225    2.083e-16   3.43e-16      0.608      0.543   -4.63e-16     8.8e-16\n",
      "rfm_quintile_233      -0.0539      0.126     -0.428      0.669      -0.301       0.193\n",
      "rfm_quintile_234      -0.0642      0.380     -0.169      0.866      -0.809       0.680\n",
      "rfm_quintile_235    2.184e-16   2.98e-16      0.733      0.463   -3.65e-16    8.02e-16\n",
      "rfm_quintile_244      -0.0813      0.129     -0.630      0.529      -0.334       0.172\n",
      "rfm_quintile_245       0.7629      0.232      3.290      0.001       0.308       1.217\n",
      "rfm_quintile_251      -0.2148      0.296     -0.727      0.467      -0.794       0.365\n",
      "rfm_quintile_252      -0.5089      0.330     -1.541      0.123      -1.156       0.139\n",
      "rfm_quintile_253      -0.1925      0.437     -0.440      0.660      -1.050       0.664\n",
      "rfm_quintile_254      -0.8087      0.526     -1.538      0.124      -1.839       0.222\n",
      "rfm_quintile_255       0.2978      0.112      2.669      0.008       0.079       0.517\n",
      "rfm_quintile_311       0.2799      0.093      3.022      0.003       0.098       0.461\n",
      "rfm_quintile_312      -0.0157      0.356     -0.044      0.965      -0.713       0.681\n",
      "rfm_quintile_315     2.59e-16   2.51e-16      1.033      0.302   -2.33e-16    7.51e-16\n",
      "rfm_quintile_322       0.3734      0.134      2.785      0.005       0.111       0.636\n",
      "rfm_quintile_323      -0.0252      0.594     -0.042      0.966      -1.189       1.138\n",
      "rfm_quintile_325    3.662e-16    1.5e-16      2.442      0.015    7.23e-17     6.6e-16\n",
      "rfm_quintile_333       0.1218      0.127      0.958      0.338      -0.128       0.371\n",
      "rfm_quintile_334       0.5932      0.323      1.837      0.066      -0.040       1.226\n",
      "rfm_quintile_335   -5.422e-16    3.1e-16     -1.749      0.080   -1.15e-15    6.55e-17\n",
      "rfm_quintile_344       0.2141      0.125      1.710      0.087      -0.031       0.460\n",
      "rfm_quintile_345       1.2237      0.210      5.841      0.000       0.813       1.634\n",
      "rfm_quintile_351      -0.1120      0.222     -0.505      0.613      -0.546       0.322\n",
      "rfm_quintile_352      -0.4969      0.578     -0.860      0.390      -1.629       0.635\n",
      "rfm_quintile_353      -0.8848      0.659     -1.343      0.179      -2.176       0.406\n",
      "rfm_quintile_354      -0.2532      0.437     -0.579      0.563      -1.110       0.604\n",
      "rfm_quintile_355       0.3806      0.117      3.251      0.001       0.151       0.610\n",
      "rfm_quintile_422       0.3384      0.094      3.617      0.000       0.155       0.522\n",
      "rfm_quintile_423       0.2768      0.228      1.212      0.226      -0.171       0.725\n",
      "rfm_quintile_425            0          0        nan        nan           0           0\n",
      "rfm_quintile_433       0.2561      0.093      2.741      0.006       0.073       0.439\n",
      "rfm_quintile_434       0.2669      0.198      1.351      0.177      -0.120       0.654\n",
      "rfm_quintile_435            0          0        nan        nan           0           0\n",
      "rfm_quintile_452      -0.2688      0.235     -1.142      0.253      -0.730       0.192\n",
      "rfm_quintile_453      -0.3669      0.256     -1.434      0.151      -0.868       0.134\n",
      "rfm_quintile_454       1.0468      0.669      1.565      0.118      -0.264       2.358\n",
      "rfm_quintile_455            0          0        nan        nan           0           0\n",
      "rfm_quintile_534       0.4340      0.683      0.635      0.525      -0.906       1.774\n",
      "rfm_quintile_544       0.3127      0.093      3.362      0.001       0.130       0.495\n",
      "rfm_quintile_545       1.2272      0.128      9.570      0.000       0.976       1.478\n",
      "rfm_quintile_554      -0.1396      0.238     -0.586      0.558      -0.606       0.327\n",
      "rfm_quintile_555       0.3494      0.092      3.780      0.000       0.168       0.531\n",
      "======================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import statsmodels.api as sm\n",
    "# Filter out perfect 0 or 1 click rates to avoid separation in logistic regression\n",
    "grouped = grouped[(grouped['click_rate'] > 0) & (grouped['click_rate'] < 1)].copy()\n",
    "\n",
    "# One-hot encode categorical predictors, drop first to avoid multicollinearity\n",
    "X = pd.get_dummies(grouped[['segment', 'channel', 'rfm_quintile']], drop_first=True)\n",
    "\n",
    "# Add intercept constant\n",
    "X = sm.add_constant(X).astype(float)\n",
    "\n",
    "# Response: number of clicks\n",
    "y = grouped['sum'] / grouped['count']\n",
    "\n",
    "\n",
    "# Frequency weights: number of emails sent per group\n",
    "weights = grouped['count'].astype(float)\n",
    "\n",
    "# Fit binomial GLM with frequency weights\n",
    "model = sm.GLM(y, X, family=sm.families.Binomial(), freq_weights=weights)\n",
    "result = model.fit()\n",
    "\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6a496",
   "metadata": {},
   "source": [
    "#### *Q5: Customer Segment Response by Channel Type — Promotions vs. Newsletters*\n",
    "\n",
    "Analysis confirms that customer segment (new vs. repeat) significantly affects click rates, with repeat customers showing higher engagement. Channel type also has a significant impact: newsletters perform worse than promotions, as indicated by the negative coefficient for the newsletter channel.\n",
    "\n",
    "Therefore, while segment drives response differences, the data supports prioritizing promotional emails over newsletters to maximize click rates within these customer segments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1200b41c",
   "metadata": {},
   "source": [
    "### Q6: What is the interplay between purchase frequency and email engagement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5735ff19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['campaign_id', 'customer_id', 'send_timestamp', 'subject_variant',\n",
       "       'opened', 'clicked', 'unsubscribed', 'purchase', 'revenue', 'channel',\n",
       "       'segment', 'day_sent', 'hour_sent', 'is_first', 'prior_purchase',\n",
       "       'days_since_last_purchase', 'days_since_last_purchase_cum',\n",
       "       'cumulative_opens', 'cumulative_opens_scaled',\n",
       "       'days_since_last_purchase_cum_scaled', 'recency', 'frequency',\n",
       "       'monetary', 'recency_inv', 'R_rank', 'R_quintile', 'F_rank',\n",
       "       'F_quintile', 'M_rank', 'M_quintile', 'RFM_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_filtered.columns\n",
    "\n",
    "## group by purchase frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "326b29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_pf_engagement = (\n",
    "    df_filtered\n",
    "    .groupby(['F_quintile', 'clicked'],observed=True)\n",
    "    .agg(\n",
    "        count_emails=('clicked', 'count'),\n",
    "        clicks=('clicked', 'sum'),\n",
    "        avg_click_rate=('clicked', 'mean'),\n",
    "        avg_revenue=('revenue', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34bd3824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square statistic: 35.62945887912943\n",
      "P-value: 3.448730717220716e-07\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "# Contingency table of frequency quintile vs clicked\n",
    "contingency = pd.crosstab(df_filtered['F_quintile'], df_filtered['clicked'])\n",
    "\n",
    "# Chi-square test of independence\n",
    "chi2, p_value, dof, expected = stats.chi2_contingency(contingency)\n",
    "\n",
    "print(f\"Chi-square statistic: {chi2}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1483a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The p-value (≈ 3.45e-7) is effectively zero, indicating a statistically significant association between purchase frequency and email engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b4a26f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA F-statistic: 4.2482048127942456\n",
      "P-value: 0.0019392522774433275\n"
     ]
    }
   ],
   "source": [
    "# ANOVA test on revenue by F_quintile groups\n",
    "groups = [group['revenue'].dropna() for name, group in df_filtered.groupby('F_quintile',observed=True)]\n",
    "anova_stat, anova_p = stats.f_oneway(*groups)\n",
    "\n",
    "print(f\"ANOVA F-statistic: {anova_stat}\")\n",
    "print(f\"P-value: {anova_p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c560976",
   "metadata": {},
   "source": [
    "The ANOVA p-value is 0.0019, below 0.05, indicating a statistically significant difference in average revenue across purchase frequency groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a6d169",
   "metadata": {},
   "source": [
    "#### *Q6: Relationship Between Purchase Frequency and Email Engagement*\n",
    "\n",
    "Statistical analysis reveals a significant difference in average revenue across purchase frequency groups, indicating that customers with varying purchase frequencies generate different revenue levels. Email engagement (click behavior) also varies significantly by purchase frequency, showing differentiated response patterns.\n",
    "\n",
    "*Conclusion:*  \n",
    "* Purchase frequency is significantly associated with both email engagement and average revenue.  \n",
    "* Higher purchase frequency customers tend to generate more revenue and engage more with emails.  \n",
    "* This analysis does not establish causation between email engagement and purchases.\n",
    "\n",
    "*Further analysis with causal or temporal models is necessary to determine if email engagement influences purchase behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5925354",
   "metadata": {},
   "source": [
    "### Q7: For engaged customers (opened/clicked), what is the uplift in next-30-day order rate and average order value vs. non-openers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11dbe0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next-30-day order rate (non-engaged): 0.0079\n",
      "Next-30-day order rate (engaged): 0.0144\n",
      "Order rate uplift: 0.0065\n",
      "Avg order value (non-engaged): £36.03\n",
      "Avg order value (engaged): £35.60\n",
      "Avg order value uplift: £-0.43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "df_filtered['engaged'] = (df_filtered['opened'] | df_filtered['clicked']).astype(int)\n",
    "df_filtered['window_end'] = df_filtered['send_timestamp'] + timedelta(days=30)\n",
    "df_filtered['customer_id'] = df_filtered['customer_id'].astype(str)\n",
    "\n",
    "purchases = df_filtered[df_filtered['purchase'] == 1][['customer_id', 'send_timestamp', 'revenue']]\n",
    "purchases = purchases.rename(columns={'send_timestamp': 'purchase_timestamp', 'revenue': 'purchase_revenue'})\n",
    "purchases['customer_id'] = purchases['customer_id'].astype(str)\n",
    "\n",
    "# Cross join filter: join on customer_id where purchase_timestamp between send_timestamp and window_end\n",
    "merged = pd.merge(df_filtered, purchases, on='customer_id', how='left')\n",
    "\n",
    "mask = (merged['purchase_timestamp'] >= merged['send_timestamp']) & (merged['purchase_timestamp'] <= merged['window_end'])\n",
    "merged = merged[mask]\n",
    "\n",
    "# Aggregate per email row\n",
    "agg = merged.groupby(['customer_id', 'send_timestamp', 'engaged']).agg(\n",
    "    total_orders=('purchase_timestamp', 'count'),\n",
    "    total_revenue=('purchase_revenue', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Join agg back to emails without purchases, fill 0\n",
    "emails = df_filtered[['customer_id', 'send_timestamp', 'engaged']].drop_duplicates()\n",
    "result = emails.merge(agg, on=['customer_id', 'send_timestamp', 'engaged'], how='left').fillna({'total_orders':0, 'total_revenue':0})\n",
    "\n",
    "summary = result.groupby('engaged').agg(\n",
    "    total_emails=('send_timestamp', 'count'),\n",
    "    total_orders=('total_orders', 'sum'),\n",
    "    total_revenue=('total_revenue', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "summary['order_rate'] = summary['total_orders'] / summary['total_emails']\n",
    "summary['avg_order_value'] = summary.apply(\n",
    "    lambda row: row['total_revenue'] / row['total_orders'] if row['total_orders'] > 0 else 0, axis=1\n",
    ")\n",
    "\n",
    "order_rate_uplift = summary.loc[summary['engaged'] == 1, 'order_rate'].values[0] - summary.loc[summary['engaged'] == 0, 'order_rate'].values[0]\n",
    "avg_order_value_uplift = summary.loc[summary['engaged'] == 1, 'avg_order_value'].values[0] - summary.loc[summary['engaged'] == 0, 'avg_order_value'].values[0]\n",
    "\n",
    "print(f\"Next-30-day order rate (non-engaged): {summary.loc[summary['engaged'] == 0, 'order_rate'].values[0]:.4f}\")\n",
    "print(f\"Next-30-day order rate (engaged): {summary.loc[summary['engaged'] == 1, 'order_rate'].values[0]:.4f}\")\n",
    "print(f\"Order rate uplift: {order_rate_uplift:.4f}\")\n",
    "\n",
    "print(f\"Avg order value (non-engaged): £{summary.loc[summary['engaged'] == 0, 'avg_order_value'].values[0]:.2f}\")\n",
    "print(f\"Avg order value (engaged): £{summary.loc[summary['engaged'] == 1, 'avg_order_value'].values[0]:.2f}\")\n",
    "print(f\"Avg order value uplift: £{avg_order_value_uplift:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ec9ce",
   "metadata": {},
   "source": [
    "#### Q7: Impact of Email Engagement on Next-30-Day Order Rate and Average Order Value\n",
    "\n",
    "Engaged customers (opened or clicked emails) show a higher next-30-day order rate (1.44%) compared to non-engaged customers (0.79%), an uplift of 0.65%. However, the average order value for engaged customers (£35.60) is slightly lower than for non-engaged customers (£36.03).\n",
    "\n",
    "This suggests engaged customers purchase more frequently but at marginally lower values per order, likely influenced by promotional offers. Non-engaged customers buy less often but spend more per transaction, potentially at regular prices.\n",
    "\n",
    "Further analysis of pricing and discount application is needed to validate this interpretation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
